{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "developing-question",
   "metadata": {},
   "source": [
    "# Active Inference Design Agent (demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organic-productivity",
   "metadata": {},
   "source": [
    "First, let us generate 10-20 audio signals from HA algorithm with different gains and within different contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "quick-constitution",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling ReactiveMP [a194aa59-28ba-4574-a09c-4a745416d6e3]\n",
      "└ @ Base loading.jl:1317\n"
     ]
    }
   ],
   "source": [
    "using Revise\n",
    "using Rocket\n",
    "using ReactiveMP\n",
    "using GraphPPL\n",
    "using Distributions\n",
    "using LinearAlgebra\n",
    "import ProgressMeter\n",
    "using WAV\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "closing-ballot",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "batch_coupled_learning (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# include auxilary functions for data processing\n",
    "include(\"helpers/aida_segmentation.jl\")\n",
    "# include SNR quality metric\n",
    "include(\"helpers/aida_snr.jl\")\n",
    "# include SNR quality metric\n",
    "include(\"helpers/aida_ar.jl\")\n",
    "# include models and corresponding inference algos\n",
    "include(\"models_inferences.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "premium-wrist",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs = 8000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rural-athens",
   "metadata": {},
   "source": [
    "In this demo we will work with two different contexts (environemtns): babble and train noises. \n",
    "\n",
    "You can think of a user who keeps wandering around a train station: sometimes train arrives and he/she hears the train noises. When there is no train arriving to the platform, the user hears the babble from people waiting for the train.\n",
    "When someone starts talking to the user he would (maybe) prefer to damp the environmental noise and focus only on the speaker.\n",
    "\n",
    "Another possible scenario you can think of is the user who steps out of the train and goes to the bar, where people produce babble noise :D "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-metro",
   "metadata": {},
   "source": [
    "## Let's obtain priors for the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "natural-pioneer",
   "metadata": {},
   "source": [
    "To make our model identifiabile, we would like to obtain priors for the environmental noises. To do this, we use Voice-Activity-Detection (VAD) to find the silent segments (with no speech). When those frames are identified, we fit them to AR model of order 2 to learn the parameters of AR.\n",
    "\n",
    "Surely, you can obtain the priors based on different logic. (to discuss (1) it doesn't have to be AR, (2) it doesn't have to be VAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "heavy-walter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "376"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load sound from .wav\n",
    "babble, fs = wavread(\"sound/AIDA/training/sp01_babble_sn0.wav\")\n",
    "# split babble into overlapping segments (default 0.01s=10ms, 0.0025=2.5ms)\n",
    "bbl_seg = get_frames(babble, fs)\n",
    "# compute number of segments\n",
    "bbl_totseg = size(bbl_seg, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "brown-brighton",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "391"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, fs = wavread(\"sound/AIDA/training/sp11_train_sn0.wav\")\n",
    "tr_seg = get_frames(train, fs)\n",
    "tr_totseg = size(tr_seg, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "sunset-lighter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Priors for contexts were obtained through running VAD-AR block\n",
    "# prior for train noise\n",
    "trmη = [0.4132104755921993 -0.16961911841949667]\n",
    "trvη = [0.00972993440498344 -0.0027103005529199036; -0.0027103005529199036 0.004281987640515784]\n",
    "# trτ  = (41.0, 0.03644943410647206)\n",
    "trτ  = (41.0, 0.05)\n",
    "\n",
    "# prior for babble noise\n",
    "bblmη = [1.1192255902602752 -0.43086292293101314]\n",
    "bblvη = [0.007837790430663492 -0.005039080815241558; -0.005039080815241558 0.00596413119195013]\n",
    "# bblτ  = (41.0, 0.0029780512310493387);\n",
    "bblτ  = (41.0, 0.05);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "leading-building",
   "metadata": {},
   "source": [
    "### VAD block\n",
    "You can skip this entire block if you used priors from the above cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abstract-scholarship",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lar_inference_ex (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(\"models/everything.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "trained-controversy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ar_ssm (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import WAV\n",
    "include(\"helpers/aida_segmentation.jl\")\n",
    "include(\"helpers/aida_ar.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "consistent-gospel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "376"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Choose the auido signal from where you intend to extract the parameters\n",
    "context_seg = bbl_seg\n",
    "totseg = bbl_totseg\n",
    "# context_seg = tr_seg\n",
    "# totseg = tr_totseg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "higher-religion",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_order = 10\n",
    "vmp_iter = 50\n",
    "fe_ar = zeros(totseg, vmp_iter)\n",
    "fe_gaussian = zeros(totseg, vmp_iter);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "alternate-baseline",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:08:47\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "ProgressMeter.@showprogress for segnum in 1:totseg\n",
    "    inputs, outputs = ar_ssm(context_seg[segnum, :], ar_order)\n",
    "    γ, τ, θ, x, fe = lar_inference_ex(outputs, ar_order, vmp_iter)\n",
    "    mθ, vθ = mean(θ), cov(θ)\n",
    "    mγ = mean(γ)\n",
    "    fe_ar[segnum, :] = fe\n",
    "    \n",
    "    x, γ, fe = inference_gaussian(outputs, vmp_iter, 1e4)\n",
    "    mx, vx = mean(x), cov(x)\n",
    "    mγ = mean(γ)\n",
    "    fe_gaussian[segnum, :] = fe\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "drawn-session",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detect silent frames based on model comparison\n",
    "vad = [x[end] < y[end] for (x, y) in zip(eachrow(fe_ar), eachrow(fe_gaussian))];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "intended-pressing",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsegs = findall(isequal(1), vad)\n",
    "nsegs = findall(isequal(0), vad);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "practical-group",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.938993996124832"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv(var(sum([context_seg[nseg, :] for nseg in nsegs])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "primary-collar",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:06\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "cmx, cvx, cmη, cvη, cτ = lar_batch_learning(hcat([context_seg[nseg, :] for nseg in nsegs]...)', 2, 10, 1e-12);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "peaceful-affiliate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.1192255902602752 -0.43086292293101314], [0.007837790430663492 -0.005039080815241558; -0.005039080815241558 0.00596413119195013], , (41.0, 0.0029780512310493387)\n"
     ]
    }
   ],
   "source": [
    "println(\"$(mean(cmη, dims=1)), $(mean(cvη, dims=1)[1, :, :]), , $((cτ[end][1], cτ[end][2]))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changed-astrology",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Priors for contexts were obtained through running VAD block\n",
    "trmη = mean(cmη, dims=1)\n",
    "trvη = mean(cvη, dims=1)[1, :, :]\n",
    "trτ  = (cτ[end][1], cτ[end][2])\n",
    "\n",
    "# prior for babble noise\n",
    "bblmη = mean(cmη, dims=1)\n",
    "bblvη = mean(cvη, dims=1)[1, :, :]\n",
    "bblτ  = (cτ[end][1], cτ[end][2]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-history",
   "metadata": {},
   "source": [
    "## Source seperation\n",
    "When priors for the contexts are indentified, we can run noise reduction algorithm based on coupled AR: AR_speech + AR_envrionment = output. This algorithm seperates speech (**z**) and noise (**x**)\n",
    "\n",
    "We will split our dataset into training and test set. We use signals from training set to learn the mapping function between the gains proposed by agent and aprraisals provided by user. Bare in mind that this split is not necessary and in theory we can get along without it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "minute-palmer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prior_to_priors (generic function with 1 method)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coupled AR model is deisgned to work with time-varying priors for both speech and environmental noise\n",
    "# prior_to_priors map \"static\" priors to the corresponding matrices with equal elements\n",
    "function prior_to_priors(mη, vη, τ, totseg)\n",
    "    ar_order = size(mη, 2)\n",
    "    rmη = zeros(totseg, ar_order)\n",
    "    rvη = zeros(totseg, ar_order, ar_order)\n",
    "    for segnum in 1:totseg\n",
    "        rmη[segnum, :], rvη[segnum, :, :] = reshape(mη, (ar_order,)), vη\n",
    "    end\n",
    "    priors_eta = rmη, rvη\n",
    "    priors_tau = [τ for _ in 1:totseg]\n",
    "    priors_eta[1], priors_eta[2], priors_tau\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "plastic-cassette",
   "metadata": {},
   "outputs": [],
   "source": [
    "trmη_arr, trvη_arr, trτ_arr = prior_to_priors(trmη, trvη, trτ, tr_totseg)\n",
    "bblmη_arr, bblvη_arr, bblτ_arr = prior_to_priors(bblmη, bblvη, bblτ, bbl_totseg);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "massive-quantity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HA_algorithm (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function HA_algorithm(segments, priors_η, priors_τ, ar_1_order, ar_2_order, vmp_its)\n",
    "    \"\"\"Source seperation based on coupled AR model. Inference is performed in batch manner\n",
    "\n",
    "       segments: segmented audio signal\n",
    "       priors_η:   matrix of means and covariances of AR coefficients (see output formal of prior_to_priors\n",
    "       priors_τ:   array of tupes contatining the prior of environmental noise precision\n",
    "       ar_1_order: order of speech signal\n",
    "       ar_2_order: order of environmental noise signal\n",
    "       vmp_its:    number of variational iterations\n",
    "    \"\"\"\n",
    "    n_sources = 2\n",
    "    totseg = size(segments, 1)\n",
    "    l      = size(segments, 2) # dimensionality of the buffer\n",
    "    \n",
    "    rmx = zeros(totseg, l)\n",
    "    rvx = zeros(totseg, l)\n",
    "    rmθ = zeros(totseg, ar_1_order)\n",
    "    rvθ = zeros(totseg, ar_1_order, ar_1_order)\n",
    "    rγ = fill(tuple(.0, .0), totseg)\n",
    "    \n",
    "    rmz = zeros(totseg, l)\n",
    "    rvz = zeros(totseg, l)\n",
    "    rmη = zeros(totseg, ar_2_order)\n",
    "    rvη = zeros(totseg, ar_2_order, ar_2_order)\n",
    "    rτ = fill(tuple(.0, .0), totseg)\n",
    "    \n",
    "    fe  = zeros(totseg, vmp_its)\n",
    "    \n",
    "    rmo = zeros(totseg, l)\n",
    "    \n",
    "    # agent proposes gains according to its beliefs\n",
    "    ProgressMeter.@showprogress for segnum in 1:totseg\n",
    "        prior_η                           = (priors_η[1][segnum, :], priors_η[2][segnum, :, :])\n",
    "        prior_τ                           = priors_τ[segnum]\n",
    "        γ, θ, zs, τ, η, xs, fe[segnum, :] = coupled_inference(segments[segnum, :], prior_η, prior_τ, ar_1_order, ar_2_order, vmp_its)\n",
    "        mz, vz                            = mean.(zs), cov.(zs)\n",
    "        mθ, vθ                            = mean(θ), cov(θ)\n",
    "        rmz[segnum, :], rvz[segnum, :]    = first.(mz), first.(vz)\n",
    "        rmθ[segnum, :], rvθ[segnum, :, :] = mθ, vθ\n",
    "        rγ[segnum]                        = shape(γ), rate(γ)\n",
    "        \n",
    "        mx, vx                            = mean.(xs), cov.(xs)\n",
    "        mη, vη                            = mean(η), cov(η)\n",
    "        rmx[segnum, :], rvx[segnum, :]    = first.(mx), first.(vx)\n",
    "        rmη[segnum, :], rvη[segnum, :, :] = mη, vη\n",
    "        rτ[segnum]                        = shape(τ), rate(τ)\n",
    "        \n",
    "        # HA part\n",
    "        speech = rmz[segnum, :]\n",
    "        noise  = rmx[segnum, :]\n",
    "        rmo[segnum, :] = speech .+ noise\n",
    "    end\n",
    "    rmz, rvz, rmθ, rvθ, rγ, rmx, rvx, rmη, rvη, rτ, fe, rmo\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-criminal",
   "metadata": {},
   "source": [
    "#### Obtain the outputs from HA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "imposed-cassette",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_sounds_fn (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return list of files from dir_name\n",
    "function get_sounds_fn(dir_name)\n",
    "    file_names = []\n",
    "    for (root, dirs, files) in walkdir(dir_name)\n",
    "        for file in files\n",
    "            push!(file_names, joinpath(root, file)) # path to files\n",
    "        end\n",
    "    end\n",
    "    file_names\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "patient-cigarette",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_files = get_sounds_fn(\"sound/AIDA/training/\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "governing-crazy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sound/AIDA/training/sp08_babble_sn0.wav\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "380×80 Matrix{Float64}:\n",
       "  0.0542314   0.0467544    0.00265511  …  -0.126011    -0.206244\n",
       "  0.033723    0.0573443    0.0773339      -0.0227058   -0.0657979\n",
       "  0.140538    0.106693     0.0360118       0.146092     0.0616169\n",
       "  0.0662252  -0.018128    -0.032075       -0.0811182    0.0431227\n",
       "  0.0480972   0.0445875    0.0498367      -0.0253609   -0.0185553\n",
       " -0.0292673  -0.0645466   -0.103793    …   0.14066      0.123875\n",
       " -0.0381481  -0.0107425    0.0104678       0.0936613    0.0746178\n",
       " -0.028077   -0.0843837   -0.0903348      -0.036256    -0.0307016\n",
       "  0.0492569   0.0639973    0.0399182       0.050264    -0.0648824\n",
       "  0.0670797   0.00317392   0.00521867      0.044496     0.0498978\n",
       " -0.101077   -0.0573138   -0.014069    …  -0.0697348   -0.0841395\n",
       "  0.0554827   0.0452589   -0.0309763       0.156255     0.0937223\n",
       " -0.0675069   0.0357677    0.0458998      -0.156713    -0.0555437\n",
       "  ⋮                                    ⋱               \n",
       "  0.0116581  -0.00210578  -0.0115665      -0.0350963   -0.0211493\n",
       " -0.0178533  -0.0216987   -0.0550859      -0.0550249   -0.0961638\n",
       "  0.0670492   0.0895108    0.0596026   …   0.0244758    0.0264901\n",
       " -0.0633869  -0.0693991   -0.0431227       0.032197     0.0103153\n",
       "  0.0513932   0.0548112    0.0653096      -0.00201422  -0.0256661\n",
       "  0.009064   -0.00671407  -0.00466933      0.0665609    0.028901\n",
       " -0.0514847   0.00173956   0.0195929      -0.0431532   -0.0391552\n",
       "  0.0169378  -0.00396741  -0.0189825   …   0.0251473   -0.0334788\n",
       " -0.0783105  -0.120151    -0.0912198       0.0107425   -0.0112003\n",
       " -0.0747703  -0.0825526   -0.0419324       0.0646992    0.10184\n",
       "  0.0134587   0.0128178    0.0318918       0.0203558    0.0274667\n",
       " -0.0343638  -0.0481277   -0.0563677       0.0          0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for how the data is organized\n",
    "training_file = training_files[8]\n",
    "println(training_file)\n",
    "speech, fs = WAV.wavread(training_file)\n",
    "speech_seg = get_frames(speech, fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-virus",
   "metadata": {},
   "source": [
    "At this stage we run our inference algorithm to seperate **z** and **x**. We write the output into *.jld* files.\n",
    "\n",
    "You don't need to run this snippet if you haven't changed the default parameters of HA_algorithm and priors (just see *sound/AIDA/separated_jld/training/*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "caring-lounge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sound/AIDA/training/sp08_babble_sn0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:03:39\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sound/AIDA/training/sp09_babble_sn0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:03:46\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sound/AIDA/training/sp10_babble_sn0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:03:20\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sound/AIDA/training/sp11_train_sn0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:03:39\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sound/AIDA/training/sp12_train_sn0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:03:26\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sound/AIDA/training/sp13_train_sn0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:03:10\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sound/AIDA/training/sp14_train_sn0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:03:39\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sound/AIDA/training/sp15_train_sn0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:03:41\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sound/AIDA/training/sp16_train_sn0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:02:54\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sound/AIDA/training/sp17_train_sn0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:03:00\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sound/AIDA/training/sp18_train_sn0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:02:51\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sound/AIDA/training/sp19_train_sn0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:03:20\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sound/AIDA/training/sp20_train_sn0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:03:14\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "import JLD\n",
    "# r for recovered\n",
    "for training_file in training_files\n",
    "    println(training_file)\n",
    "    speech, fs = WAV.wavread(training_file)\n",
    "    speech_seg = get_frames(speech, fs)\n",
    "    # choose priors\n",
    "    priors_eta = occursin(\"babble\", training_file) ? (bblmη_arr, bblvη_arr) : (trmη_arr, trvη_arr)\n",
    "    priors_tau = occursin(\"babble\", training_file) ? bblτ_arr : trτ_arr\n",
    "    # make sure that 1D of priors and speech_seg are equal\n",
    "    if size(priors_eta, 1) != size(speech_seg, 1)\n",
    "        totseg = size(speech_seg, 1)\n",
    "        priors_eta_m, priors_eta_v, priors_tau = prior_to_priors(priors_eta[1][1, :]', priors_eta[2][1, :, :], priors_tau[1], totseg)\n",
    "        priors_eta = priors_eta_m, priors_eta_v\n",
    "    end\n",
    "    rmz, rvz, rmθ, rvθ, rγ, rmx, rvx, rmη, rvη, rτ, fe, rmo = HA_algorithm(speech_seg, priors_eta, priors_tau, 10, 2, 10);\n",
    "    \n",
    "    JLD.save(\"sound/AIDA/separated_jld/training/\"*training_file[findfirst(\"sp\", training_file)[1]:end][1:end-3]*\"jld\",\n",
    "         \"rmz\", rmz, \"rvz\", rvz, \"rmθ\", rmθ, \"rvθ\", rvθ, \"rγ\", rγ, \n",
    "         \"rmx\", rmx, \"rvx\", rvx, \"rmη\", rmη, \"rvη\", rvη, \"rτ\", rτ,\n",
    "         \"fe\", fe, \"rmo\", rmo, \"filename\", training_file,\n",
    "         \"audio\", speech)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-semester",
   "metadata": {},
   "source": [
    "### Preference learning stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civil-lover",
   "metadata": {},
   "source": [
    "#### Generate outputs of HA from JLD files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "potential-posting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain saved jld files containing inference result\n",
    "training_jlds = get_sounds_fn(\"sound/AIDA/separated_jld/training/\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "pleased-expression",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize gains that agent randomly assigns to the outputs of HA\n",
    "agent_gains = [[2.0, 1.0], [1.0, 0.0], [0.5, 0.5], [0.9, 0.3], [2.5, 1.0]];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-friday",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this snippet generates new ha outputs\n",
    "for training_jld in training_jlds\n",
    "    # read file\n",
    "    d = JLD.load(training_jld)\n",
    "    filename = d[\"filename\"]\n",
    "    # extract speech\n",
    "    rmz, rmx = d[\"rmz\"], d[\"rmx\"]\n",
    "    # extract noise\n",
    "    rz, rx = get_signal(rmz, fs), get_signal(rmx, fs)\n",
    "    # pick weights \n",
    "    whgs = rand(agent_gains) # can be changed to a smarter function\n",
    "    # create output\n",
    "    ha_out = whgs[1] .* rz + whgs[2] .* rx\n",
    "    # write wav file\n",
    "    WAV.wavwrite(ha_out, fs, \"sound/AIDA/preference_learning/ha_out_$(whgs[1])_$(whgs[2])_\"*filename[findfirst(\"sp\", filename)[1]:end])\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-gauge",
   "metadata": {},
   "source": [
    "#### Create pairs (gains, context) <-> appraisals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "senior-array",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recorded gains and appraisals \n",
    "# If you want to generate new pairs, please procceed with listening\n",
    "gains = [[0.5, 0.5], [0.9, 0.3], [0.9, 0.3], [0.9, 0.3], [0.9, 0.3], [0.9, 0.3], [0.9, 0.3], [0.9, 0.3], [0.9, 0.3], [0.9, 0.3], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [2.0, 1.0], [2.0, 1.0], [2.5, 1.0], [2.5, 1.0], [2.5, 1.0]]\n",
    "appraisals = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]\n",
    "contexts = [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "brilliant-merchant",
   "metadata": {},
   "outputs": [],
   "source": [
    "prl_files = get_sounds_fn(\"sound/AIDA/preference_learning/\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liberal-belief",
   "metadata": {},
   "source": [
    "#### Listening"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-liver",
   "metadata": {},
   "source": [
    "User gets to listen new audio samples with proposed gains. After each listening he/she evaluates the performance of HA output by binary feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conventional-anger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This snippet creates interactive loop where user can like/dislike hearing aid output\n",
    "nnum = 8 # prefix for gains\n",
    "appraisals = []\n",
    "gains = []\n",
    "contexts = []\n",
    "for prl_file in prl_files\n",
    "    \n",
    "    if !occursin(\"wav\", prl_file)\n",
    "        continue\n",
    "    end\n",
    "    WAV.wavplay(prl_file)\n",
    "    println(\"How's HA output 1, 0 ?\")\n",
    "    appraisal = readline()\n",
    "    push!(appraisals, parse(Float64, appraisal))\n",
    "    \n",
    "    # extract gains routine\n",
    "    pref_id = findfirst(\"out_\", prl_file)[end]\n",
    "    gains_str = prl_file[pref_id+1:pref_id+nnum-1]\n",
    "    push!(gains, parse.(Float64, split(gains_str, \"_\")))\n",
    "    \n",
    "    # extract context routine\n",
    "    push!(contexts, occursin(\"babble\", prl_file) ? 1.0 : 0.0)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "tamil-burton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5, 0.5], [0.9, 0.3], [0.9, 0.3], [0.9, 0.3], [0.9, 0.3], [0.9, 0.3], [0.9, 0.3], [0.9, 0.3], [0.9, 0.3], [0.9, 0.3], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [1.0, 0.0], [2.0, 1.0], [2.0, 1.0], [2.5, 1.0], [2.5, 1.0], [2.5, 1.0]]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0]\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "println(gains)\n",
    "println(appraisals)\n",
    "println(contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "usual-spain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling based agent. \n",
    "include(\"agent/agent_sampling.jl\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "sweet-frank",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "learning (generic function with 2 methods)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns paramaters of a neural netwrok that provides the highest log posterior.\n",
    "function learning(gains, contexts, appraisals, N=1000; g_jitter=1e-4)\n",
    "    # adding jitter to gains might be useful when dealing with small number of observations\n",
    "    gains = [g .+ sqrt(g_jitter)*randn(length(g)) for g in gains]\n",
    "    inputs = [vcat(g, c) for (g, c) in zip(gains, contexts)]\n",
    "    ch = Turing.sample(prefernce_learning(hcat(inputs...), appraisals, 1.0), Turing.HMC(0.05, 10), N);\n",
    "    # Extract all weight and bias parameters.\n",
    "    theta = Turing.MCMCChains.group(ch, :nn_params).value;\n",
    "    # Find the index that provided the highest log posterior in the chain.\n",
    "    _, i = findmax(ch[:lp])\n",
    "    i = i.I[1]\n",
    "    θ = Float64.(theta[i, :])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "historical-prime",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mSampling: 100%|█████████████████████████████████████████| Time: 0:00:00\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "nn_params = learning(gains, contexts, appraisals);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "joint-things",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mSampling: 100%|█████████████████████████████████████████| Time: 0:00:00\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "# Alternative (in case you don't want to use learning function)\n",
    "gains₊ = [g .+ sqrt(1e-4)*randn(length(g)) for g in gains]\n",
    "inputs = [vcat(g, c) for (g, c) in zip(gains, contexts)]\n",
    "ch = Turing.sample(prefernce_learning(hcat(inputs...), appraisals, 1.0), Turing.HMC(0.05, 10), 1000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "emerging-notebook",
   "metadata": {},
   "outputs": [],
   "source": [
    "sumstats = Turing.summarize(ch, Turing.mean, Turing.std)\n",
    "mθ, vθ   = sumstats.nt.mean, sumstats.nt.std;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "artificial-jackson",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = Turing.MCMCChains.group(ch, :nn_params).value;\n",
    "# Find the index that provided the highest log posterior in the chain.\n",
    "_, i = findmax(ch[:lp])\n",
    "i = i.I[1]\n",
    "θ = theta[i, :];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "driving-quick",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for n in 1:length(appraisals)\n",
    "    push!(predictions, nn_forward(inputs[n], θ))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "breathing-accident",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for errors on training set\n",
    "appraisals_ = [rand(Distributions.Bernoulli(p[1])) for p in predictions]\n",
    "err = sum(abs.(appraisals .- appraisals_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-register",
   "metadata": {},
   "source": [
    "## Planning\n",
    "Few things must be said about the planning stage. \n",
    "First of all, the idea of planning is a reverse problem to prefernce learning. \n",
    "Given the parameters of neural network, the goal prior for the appraisal (1.0) and an informative prior for the future context (we have an idea of how the environment evolves), we want to infer the most suitable gains.\n",
    "The evolution of the context will be based on HMM model, where the observations are \n",
    "\n",
    "For illustration purposes, we will first run the inference algorithm to obtain **z** and **x**. Secondly, we run our agent that proposes gains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "parliamentary-riding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sample_gains (generic function with 2 methods)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: both Flux and Distributions export \"params\"; uses of it in module Main must be qualified\n"
     ]
    }
   ],
   "source": [
    "# infer gains\n",
    "function sample_gains(context, nn_params, N=1000)\n",
    "    ch = Turing.sample(planning(nn_params, [1.0], context), Turing.HMC(0.01, 4), N);\n",
    "    gains_context = Turing.MCMCChains.group(ch, :gs).value\n",
    "    # Find the index that provided the highest log posterior in the chain.\n",
    "    _, i = findmax(ch[:lp])\n",
    "    i = i.I[1]\n",
    "    Float64.(gains_context[i, :])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "moving-remove",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = get_sounds_fn(\"sound/AIDA/test/\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "burning-stroke",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sound/AIDA/test/sp09_train_sn0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:04:12\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sound/AIDA/test/sp10_train_sn0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:04:19\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sound/AIDA/test/sp11_babble_sn0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:05:01\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sound/AIDA/test/sp12_babble_sn0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:04:01\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sound/AIDA/test/sp13_babble_sn0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:03:17\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sound/AIDA/test/sp14_babble_sn0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:03:43\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sound/AIDA/test/sp15_babble_sn0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:03:42\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sound/AIDA/test/sp16_babble_sn0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:02:57\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sound/AIDA/test/sp17_babble_sn0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:03:01\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sound/AIDA/test/sp18_babble_sn0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:03:04\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sound/AIDA/test/sp19_babble_sn0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:03:49\u001b[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sound/AIDA/test/sp20_babble_sn0.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:03:48\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "# obtain HA output for test set (data for acting)\n",
    "using JLD\n",
    "# r for recovered\n",
    "for test_file in test_files\n",
    "    println(test_file)\n",
    "    speech, fs = WAV.wavread(test_file)\n",
    "    speech_seg = get_frames(speech, fs)\n",
    "    # choose priors\n",
    "    priors_eta = occursin(\"babble\", test_file) ? (bblmη_arr, bblvη_arr) : (trmη_arr, trvη_arr)\n",
    "    priors_tau = occursin(\"babble\", test_file) ? bblτ_arr : trτ_arr\n",
    "    \n",
    "    # make sure that 1D of priors and speech_seg are equal\n",
    "    if size(priors_eta, 1) != size(speech_seg, 1)\n",
    "        totseg = size(speech_seg, 1)\n",
    "        priors_eta_m, priors_eta_v, priors_tau = prior_to_priors(priors_eta[1][1, :]', priors_eta[2][1, :, :], priors_tau[1], totseg)\n",
    "        priors_eta = priors_eta_m, priors_eta_v\n",
    "    end\n",
    "    rmz, rvz, rmθ, rvθ, rγ, rmx, rvx, rmη, rvη, rτ, fe, rmo = HA_algorithm(speech_seg, priors_eta, priors_tau, 10, 2, 10);\n",
    "    \n",
    "    JLD.save(\"sound/AIDA/separated_jld/test/\"*test_file[findfirst(\"sp\", test_file)[1]:end][1:end-3]*\"jld\",\n",
    "         \"rmz\", rmz, \"rvz\", rvz, \"rmθ\", rmθ, \"rvθ\", rvθ, \"rγ\", rγ, \n",
    "         \"rmx\", rmx, \"rvx\", rvx, \"rmη\", rmη, \"rvη\", rvη, \"rτ\", rτ,\n",
    "         \"fe\", fe, \"rmo\", rmo, \"filename\", test_file,\n",
    "         \"audio\", speech)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "nuclear-orbit",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_jlds = get_sounds_fn(\"sound/AIDA/separated_jld/test/\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "careful-interview",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here agent plans and learns\n",
    "for test_jld in test_jlds\n",
    "    d = JLD.load(test_jld)\n",
    "    rmz, rmx = d[\"rmz\"], d[\"rmx\"]\n",
    "    filename = d[\"filename\"]\n",
    "    context = occursin(\"babble\", filename) ? 1.0 : 0.0\n",
    "    rz, rx = get_signal(rmz, fs), get_signal(rmx, fs)\n",
    "    gs = sample_gains(context, Float64.(θ))\n",
    "    ha_out = gs[1] .* rz + gs[2] .* rx\n",
    "    # TODO: we can learn here as well. If the user provides a feedback to each proposal, we can feed this information back to our learning function.\n",
    "    WAV.wavwrite(ha_out, fs, \"sound/AIDA/planning/ha_out_$(gs[1])_$(gs[2])_\"*filename[findfirst(\"sp\", filename)[1]:end])\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
