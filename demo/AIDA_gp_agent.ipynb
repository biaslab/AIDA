{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "polish-coral",
   "metadata": {},
   "source": [
    "# Active Inference Design Agent (demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fifty-newfoundland",
   "metadata": {},
   "outputs": [],
   "source": [
    "using JLD\n",
    "using Revise\n",
    "using Rocket\n",
    "using ReactiveMP\n",
    "using GraphPPL\n",
    "using Distributions\n",
    "using LinearAlgebra\n",
    "using Random \n",
    "import ProgressMeter\n",
    "using WAV\n",
    "using Plots\n",
    "using ImageCore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "clear-laser",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "batch_coupled_learning (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# include auxilary functions for data processing\n",
    "include(\"helpers/aida_segmentation.jl\")\n",
    "# include SNR quality metric\n",
    "include(\"helpers/aida_snr.jl\")\n",
    "# include SNR quality metric\n",
    "include(\"helpers/aida_ar.jl\")\n",
    "# include models and corresponding inference algos\n",
    "include(\"models_inferences.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "precise-enterprise",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lar_inference_ex (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include(\"models/everything.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "difficult-reservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing flows agent\n",
    "#include(\"agent/agent_Bart/flow_agent.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aquatic-afghanistan",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_sounds_fn (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return list of files from dir_name\n",
    "function get_sounds_fn(dir_name)\n",
    "    file_names = []\n",
    "    for (root, dirs, files) in walkdir(dir_name)\n",
    "        for file in files\n",
    "            push!(file_names, joinpath(root, file)) # path to files\n",
    "        end\n",
    "    end\n",
    "    file_names\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "common-mapping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs = 8000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-exemption",
   "metadata": {},
   "source": [
    "In this demo we will work with two different contexts (environemtns): babble and train noises. \n",
    "\n",
    "You can think of a user who keeps wandering around a train station: sometimes train arrives and he/she hears the train noises. When there is no train arriving to the platform, the user hears the babble from people waiting for the train.\n",
    "When someone starts talking to the user he would (maybe) prefer to damp the environmental noise and focus only on the speaker.\n",
    "\n",
    "Another possible scenario you can think of is the user who steps out of the train and goes to the bar, where people produce babble noise :D "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-category",
   "metadata": {},
   "source": [
    "## Let's obtain priors for the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-whale",
   "metadata": {},
   "source": [
    "To make our model identifiabile, we would like to obtain priors for the environmental noises. To do this, we use Voice-Activity-Detection (VAD) to find the silent segments (with no speech). When those frames are identified, we fit them to AR model of order 2 to learn the parameters of AR.\n",
    "\n",
    "Surely, you can obtain the priors based on different logic. (to discuss (1) it doesn't have to be AR, (2) it doesn't have to be VAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "advised-journey",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "376"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load sound from .wav\n",
    "babble, fs = wavread(\"sound/AIDA/training/babble/0dB/sp01_babble_sn0.wav\")\n",
    "# split babble into overlapping segments (default 0.01s=10ms, 0.0025=2.5ms)\n",
    "bbl_seg = get_frames(babble, fs)\n",
    "# compute number of segments\n",
    "bbl_totseg = size(bbl_seg, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "driven-metallic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "376"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, fs = wavread(\"sound/AIDA/training/train/0dB/sp01_train_sn0.wav\")\n",
    "tr_seg = get_frames(train, fs)\n",
    "tr_totseg = size(tr_seg, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "attractive-chance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Priors for contexts were obtained through running VAD-AR block\n",
    "# prior for train noise\n",
    "trmη = [0.34546575880691316 -0.16774278649436555]\n",
    "trvη = [0.00972993440498344 -0.0027103005529199036; -0.0027103005529199036 0.004281987640515784]\n",
    "# trτ  = (41.0, 0.03644943410647206)\n",
    "trτ  = (41.0, 0.05)\n",
    "\n",
    "# prior for babble noise\n",
    "bblmη = [1.1192255902602752 -0.43086292293101314]\n",
    "bblvη = [0.007837790430663492 -0.005039080815241558; -0.005039080815241558 0.00596413119195013]\n",
    "# bblτ  = (41.0, 0.0029780512310493387);\n",
    "bblτ  = (41.0, 0.05);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prospective-cookie",
   "metadata": {},
   "source": [
    "## Source seperation\n",
    "When priors for the contexts are indentified, we can run noise reduction algorithm based on coupled AR: AR_speech + AR_envrionment = output. This algorithm seperates speech (**z**) and noise (**x**)\n",
    "\n",
    "We will split our dataset into training and test set. We use signals from training set to learn the mapping function between the gains proposed by agent and aprraisals provided by user. Bare in mind that this split is not necessary and in theory we can get along without it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "vocational-wright",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prior_to_priors (generic function with 1 method)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# coupled AR model is deisgned to work with time-varying priors for both speech and environmental noise\n",
    "# prior_to_priors map \"static\" priors to the corresponding matrices with equal elements\n",
    "function prior_to_priors(mη, vη, τ, totseg)\n",
    "    ar_order = size(mη, 2)\n",
    "    rmη = zeros(totseg, ar_order)\n",
    "    rvη = zeros(totseg, ar_order, ar_order)\n",
    "    for segnum in 1:totseg\n",
    "        rmη[segnum, :], rvη[segnum, :, :] = reshape(mη, (ar_order,)), vη\n",
    "    end\n",
    "    priors_eta = rmη, rvη\n",
    "    priors_tau = [τ for _ in 1:totseg]\n",
    "    priors_eta[1], priors_eta[2], priors_tau\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "disturbed-vampire",
   "metadata": {},
   "outputs": [],
   "source": [
    "trmη_arr, trvη_arr, trτ_arr = prior_to_priors(trmη, trvη, trτ, tr_totseg)\n",
    "bblmη_arr, bblvη_arr, bblτ_arr = prior_to_priors(bblmη, bblvη, bblτ, bbl_totseg);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "hairy-harmony",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HA_algorithm (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function HA_algorithm(segments, priors_η, priors_τ, ar_1_order, ar_2_order, vmp_its)\n",
    "    \"\"\"Source seperation based on coupled AR model. Inference is performed in batch manner\n",
    "\n",
    "       segments: segmented audio signal\n",
    "       priors_η:   matrix of means and covariances of AR coefficients (see output formal of prior_to_priors\n",
    "       priors_τ:   array of tupes contatining the prior of environmental noise precision\n",
    "       ar_1_order: order of speech signal\n",
    "       ar_2_order: order of environmental noise signal\n",
    "       vmp_its:    number of variational iterations\n",
    "    \"\"\"\n",
    "    n_sources = 2\n",
    "    totseg = size(segments, 1)\n",
    "    l      = size(segments, 2) # dimensionality of the buffer\n",
    "    \n",
    "    rmx = zeros(totseg, l)\n",
    "    rvx = zeros(totseg, l)\n",
    "    rmθ = zeros(totseg, ar_1_order)\n",
    "    rvθ = zeros(totseg, ar_1_order, ar_1_order)\n",
    "    rγ = fill(tuple(.0, .0), totseg)\n",
    "    \n",
    "    rmz = zeros(totseg, l)\n",
    "    rvz = zeros(totseg, l)\n",
    "    rmη = zeros(totseg, ar_2_order)\n",
    "    rvη = zeros(totseg, ar_2_order, ar_2_order)\n",
    "    rτ = fill(tuple(.0, .0), totseg)\n",
    "    \n",
    "    fe  = zeros(totseg, vmp_its)\n",
    "    \n",
    "    rmo = zeros(totseg, l)\n",
    "    \n",
    "    # agent proposes gains according to its beliefs\n",
    "    ProgressMeter.@showprogress for segnum in 1:totseg\n",
    "        prior_η                           = (priors_η[1][segnum, :], priors_η[2][segnum, :, :])\n",
    "        prior_τ                           = priors_τ[segnum]\n",
    "        γ, θ, zs, τ, η, xs, fe[segnum, :] = coupled_inference(segments[segnum, :], prior_η, prior_τ, ar_1_order, ar_2_order, vmp_its)\n",
    "        mz, vz                            = mean.(zs), cov.(zs)\n",
    "        mθ, vθ                            = mean(θ), cov(θ)\n",
    "        rmz[segnum, :], rvz[segnum, :]    = first.(mz), first.(vz)\n",
    "        rmθ[segnum, :], rvθ[segnum, :, :] = mθ, vθ\n",
    "        rγ[segnum]                        = shape(γ), rate(γ)\n",
    "        \n",
    "        mx, vx                            = mean.(xs), cov.(xs)\n",
    "        mη, vη                            = mean(η), cov(η)\n",
    "        rmx[segnum, :], rvx[segnum, :]    = first.(mx), first.(vx)\n",
    "        rmη[segnum, :], rvη[segnum, :, :] = mη, vη\n",
    "        rτ[segnum]                        = shape(τ), rate(τ)\n",
    "        \n",
    "        # HA part\n",
    "        speech = rmz[segnum, :]\n",
    "        noise  = rmx[segnum, :]\n",
    "        rmo[segnum, :] = speech .+ noise\n",
    "    end\n",
    "    rmz, rvz, rmθ, rvθ, rγ, rmx, rvx, rmη, rvη, rτ, fe, rmo\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-tulsa",
   "metadata": {},
   "source": [
    "#### Obtain the outputs from HA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desirable-travel",
   "metadata": {},
   "source": [
    "At this stage we run our inference algorithm to seperate **z** and **x**. We write the output into *.jld* files.\n",
    "\n",
    "You don't need to run this snippet if you haven't changed the default parameters of HA_algorithm and priors (just see *sound/AIDA/separated_jld/training/*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "close-catholic",
   "metadata": {},
   "source": [
    "### Preference learning stage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assumed-consent",
   "metadata": {},
   "source": [
    "#### Listening"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-michael",
   "metadata": {},
   "source": [
    "User gets to listen new audio samples with proposed gains. After each listening he/she evaluates the performance of HA output by binary feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "auburn-publicity",
   "metadata": {},
   "source": [
    "## Planning\n",
    "Few things must be said about the planning stage. \n",
    "First of all, the idea of planning is a reverse problem to prefernce learning. \n",
    "Given the parameters of neural network, the goal prior for the appraisal (1.0) and an informative prior for the future context (we have an idea of how the environment evolves), we want to infer the most suitable gains.\n",
    "The evolution of the context will be based on HMM model, where the observations are \n",
    "\n",
    "For illustration purposes, we will first run the inference algorithm to obtain **z** and **x**. Secondly, we run our agent that proposes gains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "hydraulic-graphic",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_files = get_sounds_fn(\"sound/AIDA/test/\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "accurate-diploma",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_jlds = get_sounds_fn(\"sound/AIDA/separated_jld/test/\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "located-dallas",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shuffle!(test_jlds);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-soccer",
   "metadata": {},
   "source": [
    "## Train an agent here\n",
    "Simulation is initialised with random values and a negative response. This should be updated in case it's actually close to good settings. Play with ``n_steps`` for a more finegrained search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aquatic-christianity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_new_proposal (generic function with 1 method)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load helper functions\n",
    "include(\"utils.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ranking-peter",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×1 Matrix{Float64}:\n",
       " 0.9445637143824641\n",
       " 0.29427224077866687"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndims = 2\n",
    "npoints = 1\n",
    "\n",
    "n_steps = 20\n",
    "gridticks =LinRange(0,2,n_steps)\n",
    "grid = Iterators.product(gridticks,gridticks)\n",
    "\n",
    "# Initial data point for train context\n",
    "x1_train = rand(ndims,npoints) \n",
    "y1_train = [0.]\n",
    "\n",
    "# Initial data point for babble context\n",
    "x1_babble = rand(ndims,npoints) \n",
    "y1_babble = [0.]\n",
    "\n",
    "# Parameters for train context. Just random numbers atm\n",
    "σ_train = 0.2\n",
    "l_train = 0.5\n",
    "\n",
    "# Parameters for babble context\n",
    "σ_babble = 0.2\n",
    "l_babble = 0.5\n",
    "\n",
    "\n",
    "\n",
    "# Keep track of last point visited. This is the position of the agent in param space\n",
    "current_train = x1_train\n",
    "current_babble = x1_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "respective-governor",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How's HA output 0..1 ?\n",
      "stdin> 0\n",
      "How's HA output 0..1 ?\n",
      "stdin> 0\n",
      "How's HA output 0..1 ?\n",
      "stdin> 0\n",
      "How's HA output 0..1 ?\n",
      "stdin> 0\n",
      "How's HA output 0..1 ?\n",
      "stdin> 0\n",
      "How's HA output 0..1 ?\n",
      "stdin> 0\n",
      "How's HA output 0..1 ?\n",
      "stdin> 0\n",
      "How's HA output 0..1 ?\n",
      "stdin> 1\n",
      "How's HA output 0..1 ?\n",
      "stdin> 0\n",
      "How's HA output 0..1 ?\n",
      "stdin> 1\n",
      "How's HA output 0..1 ?\n",
      "stdin> 1\n",
      "How's HA output 0..1 ?\n",
      "stdin> 0\n",
      "How's HA output 0..1 ?\n",
      "stdin> 0\n",
      "How's HA output 0..1 ?\n",
      "stdin> 0\n",
      "How's HA output 0..1 ?\n",
      "stdin> 0\n",
      "How's HA output 0..1 ?\n",
      "stdin> 1\n",
      "How's HA output 0..1 ?\n",
      "stdin> 0\n",
      "How's HA output 0..1 ?\n",
      "stdin> 0\n",
      "How's HA output 0..1 ?\n",
      "stdin> 0\n",
      "How's HA output 0..1 ?\n",
      "stdin> 1\n"
     ]
    }
   ],
   "source": [
    "# Simulation loops\n",
    "for t in 1:length(test_jlds)\n",
    "    test_jld = test_jlds[t]\n",
    "    d = JLD.load(test_jld)\n",
    "    rmz, rmx = d[\"rmz\"], d[\"rmx\"]\n",
    "    filename = d[\"filename\"]\n",
    "    # TODO: context inference\n",
    "    \n",
    "    if occursin(\"/babble/\", filename)\n",
    "        context = 1.0\n",
    "        \n",
    "        if t % 3 == 0 # This should be done in a smarter way\n",
    "            σ_babble,l_babble = optimize_hyperparams(x1_babble,y1_babble,[σ_babble,l_babble])\n",
    "        end\n",
    "        # Get new proposal and update context specific state\n",
    "        x2 = get_new_proposal(grid,x1_babble,y1_babble,current_babble,σ_babble,l_babble)\n",
    "        current_babble = x2\n",
    "        \n",
    "    elseif occursin(\"/train/\", filename)\n",
    "        context = 0.0\n",
    "        \n",
    "        if t % 3 == 0\n",
    "            σ_train,l_train = optimize_hyperparams(x1_train,y1_train,[σ_train,l_train])\n",
    "        end\n",
    "        \n",
    "        # Get new proposal and update context specific state\n",
    "        x2 = get_new_proposal(grid,x1_train,y1_train,current_train,σ_train,l_train)\n",
    "        current_train = x2\n",
    "\n",
    "    else\n",
    "        println(\"Wrong file encountered\")\n",
    "        break\n",
    "    end\n",
    "   \n",
    "    # Get the signal\n",
    "    rz, rx = get_signal(rmz, fs), get_signal(rmx, fs)\n",
    " \n",
    "    # Apply params\n",
    "    ha_out = x2[1] .* rz + x2[2] .* rx\n",
    "    \n",
    "    # Let user listen and request feedback\n",
    "    full_name = \"sound/AIDA/planning/ha_out_$(x2[1])_$(x2[2])_\"*filename[findfirst(\"sp\", filename)[1]:end]\n",
    "    WAV.wavwrite(ha_out, fs, full_name)\n",
    "    WAV.wavplay(full_name)\n",
    "    println(\"How's HA output 0..1 ?\")\n",
    "    appraisal = readline()\n",
    "    \n",
    "    # Add to GP dataset\n",
    "    if context == 1.0\n",
    "        y1_train = vcat(y1_train,parse(Int64,appraisal))\n",
    "        x1_train = hcat(x1_train,collect(x2))\n",
    "    elseif context == 0.0\n",
    "        y1_babble = vcat(y1_babble,parse(Int64,appraisal))\n",
    "        x1_babble = hcat(x1_babble,collect(x2))\n",
    "    end\n",
    "\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "historical-channel",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.0",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
